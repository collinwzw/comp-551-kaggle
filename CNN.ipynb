{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import sys\n",
    "stderr = sys.stderr\n",
    "sys.stderr = open(os.devnull, 'w')\n",
    "import keras\n",
    "sys.stderr = stderr\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils import plot_model\n",
    "from keras.models import load_model\n",
    "\n",
    "image_colmn = 100\n",
    "image_row  = 100\n",
    "read_train_filename = './all/train_images.npy'\n",
    "\n",
    "train_label_filename = './all/train_labels.csv'\n",
    "train_data_filename = './train_data.npy'\n",
    "class_name_list =[\"sink\",\"pear\",\"moustache\",\"nose\",\"skateboard\",\"penguin\",\"peanut\",\"skull\",\"panda\",\"paintbrush\",\"nail\",\"apple\",\"rifle\",\"mug\",\"sailboat\",\"pineapple\",\"spoon\",\"rabbit\",\"shovel\",\"rollerskates\",\"screwdriver\",\"scorpion\",\"rhinoceros\",\"pool\",\"octagon\",\"pillow\",\"parrot\",\"squiggle\",\"mouth\",\"empty\",\"pencil\"]\n",
    "num_classes = 31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    class_label_name=[]\n",
    "    with open (train_label_filename,'r',) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        for row in readCSV:\n",
    "            \n",
    "            class_label_name.append(row)\n",
    "    del class_label_name[0]\n",
    "    class_id_list = []\n",
    "    for i in range(len(class_name_list)):\n",
    "        class_id_list.append(i)\n",
    "        \n",
    "    hashmap = dict(zip(class_name_list,class_id_list))\n",
    "    class_label_vector=[]\n",
    "\n",
    "    for r in tqdm(class_label_name):\n",
    "        word = r[1]\n",
    "        if word in hashmap:\n",
    "            class_label_vector.append(hashmap[word])\n",
    "    images = np.load(read_train_filename,encoding='latin1')\n",
    "    train_data=[]\n",
    "    for i in tqdm(range(len(images))):\n",
    "            train_data.append([np.array((images[i][1]).reshape(1,10000)),np.array(class_label_vector[i])])\n",
    "    np.save('train_data.npy',train_data)\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_transform(image, alpha, sigma, random_state=None):\n",
    "    \"\"\"Elastic deformation of images as described in [Simard2003]_.\n",
    "    .. [Simard2003] Simard, Steinkraus and Platt, \"Best Practices for\n",
    "       Convolutional Neural Networks applied to Visual Document Analysis\", in\n",
    "       Proc. of the International Conference on Document Analysis and\n",
    "       Recognition, 2003.\n",
    "    \"\"\"\n",
    "    assert len(image.shape) == 2\n",
    "\n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape\n",
    "\n",
    "    dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "    dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "\n",
    "    x, y = np.meshgrid(np.arange(shape[0]), np.arange(shape[1]), indexing='ij')\n",
    "    indices = np.reshape(x + dx, (-1, 1)), np.reshape(y + dy, (-1, 1))\n",
    "\n",
    "    return map_coordinates(image, indices, order=1).reshape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_input(x):\n",
    "    return (x - mean_px) / std_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exist\n"
     ]
    }
   ],
   "source": [
    "#checking processed data is exist.\n",
    "if (os.path.isfile(train_data_filename)==False):\n",
    "    train_data = create_train_data()\n",
    "        \n",
    "else:\n",
    "    print(\"file already exist\")\n",
    "    train_data = np.load(train_data_filename,encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 71557.94it/s]\n"
     ]
    }
   ],
   "source": [
    "read_test_filename = './all/test_images.npy'\n",
    "\n",
    "test_data_filename = './test_data.npy'\n",
    "\n",
    "if (os.path.isfile(test_data_filename)==False):\n",
    "    images = np.load(read_test_filename,encoding='latin1')\n",
    "    predict_data=[]\n",
    "    for i in tqdm(range(len(images))):\n",
    "            predict_data.append([np.array((images[i][1])).reshape(1,10000)])\n",
    "    np.save('test_data.npy',predict_data)\n",
    "    \n",
    "else:\n",
    "    print(\"file already exist\")\n",
    "    predict_data = np.load(test_data_filename,encoding='latin1')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(predict_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1, 10000)\n",
      "(10000, 100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "aug_epochs = 1\n",
    "test_size = 0.1\n",
    "#seed \n",
    "random_seed = 1\n",
    "learning_rate=0.001\n",
    "version =1\n",
    "batch_size=64\n",
    "epochs=20\n",
    "\n",
    "\n",
    "y_raw = [i[1] for i in train_data]\n",
    "#print(y_raw)\n",
    "#for y_e in y_raw:\n",
    "#    y.append(convert_y_to_vector(y_e))\n",
    "train_y=np.asarray(y_raw,dtype=float)\n",
    "\n",
    "#print(y)\n",
    "\n",
    "train_y = train_y.reshape((-1,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_x = np.array([i[0] for i in train_data])\n",
    "train_x = train_x.reshape((-1,image_colmn,image_row))\n",
    "input_shape = (image_colmn, image_row, 1)\n",
    "predict_x = np.array([i[0] for i in predict_data])\n",
    "\n",
    "predict_x =predict_x.reshape((-1,image_colmn,image_row,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "distort_1 = np.zeros(shape=train_x.shape, dtype='float32')\n",
    "distort_2 = np.zeros(shape=train_x.shape, dtype='float32')\n",
    "distort_3 = np.zeros(shape=train_x.shape, dtype='float32')\n",
    "\n",
    "for i in range(train_x.shape[0]):\n",
    "        #print(i)\n",
    "        distort_1[i] = elastic_transform(image=train_x[i], alpha=36, sigma=10)\n",
    "        distort_2[i] = elastic_transform(image=train_x[i], alpha=36, sigma=8)\n",
    "        distort_3[i] = elastic_transform(image=train_x[i], alpha=36, sigma=6)\n",
    "train_x = np.concatenate((train_x, distort_1, distort_2, distort_3), axis=0)\n",
    "train_y = np.concatenate((train_y, train_y, train_y, train_y), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x.reshape((-1,image_colmn,image_row,1))\n",
    "train_y = train_y.reshape((train_y.shape[0],))\n",
    "train_x = train_x.astype('float32')\n",
    "predict_x = predict_x.astype('float32')\n",
    "train_x /= 255\n",
    "predict_x /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "print(predict_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size=test_size, random_state=111)\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "mean_px = x_train.mean().astype(np.float32)\n",
    "std_px = x_train.std().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "        Lambda(norm_input, input_shape=input_shape),\n",
    "\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        # Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        # Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        # Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        # BatchNormalization(),\n",
    "        # MaxPooling2D(pool_size=(2, 2)),\n",
    "        # Dropout(0.25),\n",
    "\n",
    "        Flatten(),\n",
    "\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        # Dense(512, activation='relu'),\n",
    "        # BatchNormalization(),\n",
    "        # Dropout(0.5),\n",
    "        # Dense(512, activation='relu'),\n",
    "        # BatchNormalization(),\n",
    "        # Dropout(0.5),\n",
    "        Dense(31, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "original = sys.stdout\n",
    "sys.stdout = open('history_' + str(version) + '.txt', 'w')\n",
    "\n",
    "model.summary()\n",
    "history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f11aa1e8160>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, shear_range=0.3,\n",
    "                             height_shift_range=0.1, zoom_range=0.08)  # changed from 8, 0.08, 0.3, 0.08\n",
    "batches = gen.flow(x_train, y_train, batch_size=batch_size)\n",
    "val_batches = gen.flow(x_test, y_test, batch_size=batch_size)\n",
    "model.fit_generator(batches, steps_per_epoch=x_train.shape[0] // batch_size, epochs=aug_epochs,\n",
    "                        validation_data=val_batches, validation_steps=x_test.shape[0] // batch_size,\n",
    "                        use_multiprocessing=False, class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "json_name = \"model_\" + str(version) + \".json\"\n",
    "h5_name = \"model_\" + str(version) + \".h5\"\n",
    "with open(json_name, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(h5_name)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result = model.predict_classes(predict_x, batch_size=batch_size, verbose=0)\n",
    "with open ('submission.csv','w',) as csvfile: \n",
    "    csvfile.write('Id')\n",
    "    csvfile.write(\",\")\n",
    "    csvfile.write('Category')\n",
    "    csvfile.write('\\n')\n",
    "    for i in range(len(predict_result)):\n",
    "            csvfile.write('%d'%i)\n",
    "            csvfile.write(\",\")\n",
    "            csvfile.write(class_name_list[predict_result[i]])\n",
    "            csvfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
